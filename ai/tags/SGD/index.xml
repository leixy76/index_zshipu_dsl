<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>SGD on 知识铺的博客</title>
    <link>https://index.zshipu.com/ai/tags/SGD/</link>
    <description>Recent content in SGD on 知识铺的博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Wed, 18 Dec 2024 08:04:24 +0000</lastBuildDate>
    <atom:link href="https://index.zshipu.com/ai/tags/SGD/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>随机梯度下降法（Stochastic gradient descent SGD）- AI全书 -- 知识铺 -- 知识铺</title>
      <link>https://index.zshipu.com/ai/post/20241218/%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95Stochastic-gradient-descent-SGD-AI%E5%85%A8%E4%B9%A6--%E7%9F%A5%E8%AF%86%E9%93%BA--%E7%9F%A5%E8%AF%86%E9%93%BA/</link>
      <pubDate>Wed, 18 Dec 2024 08:04:24 +0000</pubDate>
      <guid>https://index.zshipu.com/ai/post/20241218/%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95Stochastic-gradient-descent-SGD-AI%E5%85%A8%E4%B9%A6--%E7%9F%A5%E8%AF%86%E9%93%BA--%E7%9F%A5%E8%AF%86%E9%93%BA/</guid>
      <description>随机梯度下降（SGD）是一种迭代优化方法，用于最小化可微分目标函数。与标准梯度下降不同，SGD在每次迭代中仅使用一个或少量样本来计算梯度，而不是整个训练集。这种方法的随机性来源于样本的随机选择或混洗，而不是按照训练集中出现的顺序选择。SGD被认为是随机的，因为它依赖于随机样本来估</description>
    </item>
  </channel>
</rss>
