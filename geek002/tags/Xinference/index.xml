<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Xinference on 知识铺的博客</title>
    <link>https://index.zshipu.com/geek002/tags/Xinference/</link>
    <description>Recent content in Xinference on 知识铺的博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Tue, 29 Oct 2024 07:56:06 +0000</lastBuildDate>
    <atom:link href="https://index.zshipu.com/geek002/tags/Xinference/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>运用Dify、Xinference与Ollama构建知识库 -- 知识铺</title>
      <link>https://index.zshipu.com/geek002/post/202410/%E8%BF%90%E7%94%A8DifyXinference%E4%B8%8EOllama%E6%9E%84%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93--%E7%9F%A5%E8%AF%86%E9%93%BA/</link>
      <pubDate>Tue, 29 Oct 2024 07:55:06 +0000</pubDate>
      <guid>https://index.zshipu.com/geek002/post/202410/%E8%BF%90%E7%94%A8DifyXinference%E4%B8%8EOllama%E6%9E%84%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93--%E7%9F%A5%E8%AF%86%E9%93%BA/</guid>
      <description>记录下运用Dify + xinference + ollama打造带重排序(Rerank)步骤的知识库问答，更好的是&amp;mdash;-即使在我的3060M上其也能完全本地运行并有不错的效果！3060M本地运行llama3-9B的生成速度参照前文。 git clone https://github.com/langgenius/dify.git cd dify/docker docker compose up -d 随后访问本地的http://localhos</description>
    </item>
    <item>
      <title>安装Dify并集成Ollama和Xinference -- 知识铺</title>
      <link>https://index.zshipu.com/geek002/post/202410/%E5%AE%89%E8%A3%85Dify%E5%B9%B6%E9%9B%86%E6%88%90Ollama%E5%92%8CXinference--%E7%9F%A5%E8%AF%86%E9%93%BA/</link>
      <pubDate>Tue, 29 Oct 2024 07:53:06 +0000</pubDate>
      <guid>https://index.zshipu.com/geek002/post/202410/%E5%AE%89%E8%A3%85Dify%E5%B9%B6%E9%9B%86%E6%88%90Ollama%E5%92%8CXinference--%E7%9F%A5%E8%AF%86%E9%93%BA/</guid>
      <description>liuqianglong.com 本文介绍了通过Docker安装Dify，然后集成Ollama和XInference，并利用Dify快速搭建一个基于知识库问答的应用。 一、Dify简介 Dify是一款开源的大语言模型（LLM）应用开发平台，旨在帮助开发者快速构建和部署生成式AI应用。以下是Dify的主要功能和特点[</description>
    </item>
    <item>
      <title>Xinference本地运行大模型bge-reranker-v2-m3教程 -- 知识铺</title>
      <link>https://index.zshipu.com/geek002/post/202410/Xinference%E6%9C%AC%E5%9C%B0%E8%BF%90%E8%A1%8C%E5%A4%A7%E6%A8%A1%E5%9E%8Bbge-reranker-v2-m3%E6%95%99%E7%A8%8B--%E7%9F%A5%E8%AF%86%E9%93%BA/</link>
      <pubDate>Tue, 29 Oct 2024 07:46:06 +0000</pubDate>
      <guid>https://index.zshipu.com/geek002/post/202410/Xinference%E6%9C%AC%E5%9C%B0%E8%BF%90%E8%A1%8C%E5%A4%A7%E6%A8%A1%E5%9E%8Bbge-reranker-v2-m3%E6%95%99%E7%A8%8B--%E7%9F%A5%E8%AF%86%E9%93%BA/</guid>
      <description>liuqianglong.com Xinference 本地运行大模型 本文介绍了如何使用 Docker 部署 Xinference 推理框架，并演示了启动和运行多种大模型的过程，包括大语言模型、图像生成模型以及多模态模型。同时，还提供了关于嵌入和重排模型的启动指导，为后续 Dify 调用这些模型打下基础。 一、Xinference 简介 Xorbits Inference (Xinference) 是一个开源分布式推理框架，专为执行大规</description>
    </item>
  </channel>
</rss>
